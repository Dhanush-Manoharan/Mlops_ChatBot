"""
Automated Model Retraining Pipeline
Triggered when performance degrades or data drift detected
"""
import os
import sys
import logging
from datetime import datetime
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.rag_pipeline import PropBotRAG
from monitoring.metrics import get_metrics_collector
from monitoring.drift_detector import get_drift_detector
from notifications.alerts import get_notification_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelRetrainingPipeline:
    """Automated model retraining pipeline"""

    def __init__(self):
        self.metrics_collector = get_metrics_collector()
        self.drift_detector = get_drift_detector()
        self.notification_manager = get_notification_manager()

        self.retraining_job_id = f"retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info(f"ğŸš€ Initializing retraining job: {self.retraining_job_id}")

    def pull_latest_data(self):
        """Pull latest data from sources"""
        logger.info("ğŸ“Š Step 1/5: Pulling latest data...")

        try:
            # In production, this would:
            # 1. Pull from DVC: dvc pull
            # 2. Download from GCS bucket
            # 3. Query latest from database

            # For now, verify ChromaDB is accessible
            from chromadb import PersistentClient
            chroma_path = os.getenv('CHROMA_PATH', '/app/chroma_data')
            client = PersistentClient(path=chroma_path)
            collections = client.list_collections()

            logger.info(f"âœ… Verified {len(collections)} ChromaDB collections available")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to pull latest data: {e}")
            return False

    def retrain_model(self):
        """Retrain the model with latest data"""
        logger.info("ğŸ”„ Step 2/5: Retraining model...")

        try:
            # In production, this would:
            # 1. Re-generate embeddings for new data
            # 2. Fine-tune GPT-4o-mini prompts based on recent queries
            # 3. Update RAG retrieval parameters
            # 4. Optimize query routing logic

            # For RAG systems, "retraining" means:
            # - Updating vector embeddings
            # - Refreshing ChromaDB indices
            # - Tuning retrieval k parameters
            # - Updating system prompts based on performance

            # Simulate retraining process
            rag = PropBotRAG()
            logger.info(f"âœ… RAG pipeline reinitialized with latest data")
            logger.info(f"   Collections: {len(rag.collection_names)}")

            return True

        except Exception as e:
            logger.error(f"âŒ Retraining failed: {e}")
            return False

    def validate_new_model(self):
        """Validate retrained model on test set"""
        logger.info("ğŸ§ª Step 3/5: Validating retrained model...")

        try:
            # In production, this would:
            # 1. Run test queries
            # 2. Compare metrics with baseline
            # 3. Check for regressions

            # Simulate validation with test queries
            test_queries = [
                "Show me properties in Back Bay",
                "What is the crime rate in Roxbury?",
                "Find 3 bedroom homes under 700k"
            ]

            rag = PropBotRAG()
            successful = 0

            for query in test_queries:
                try:
                    result = rag.chat(query, conversation_id="validation_test")
                    if result.get('answer'):
                        successful += 1
                except:
                    pass

            validation_score = (successful / len(test_queries)) * 100
            logger.info(f"âœ… Validation complete: {validation_score:.1f}% success rate")

            # Pass if > 80% success rate
            return validation_score > 80

        except Exception as e:
            logger.error(f"âŒ Validation failed: {e}")
            return False

    def compare_with_baseline(self):
        """Compare new model with current production model"""
        logger.info("ğŸ“Š Step 4/5: Comparing with baseline...")

        try:
            current_metrics = self.metrics_collector.get_metrics()

            # In production, would compare:
            # - Success rate: new vs old
            # - Response time: new vs old
            # - User satisfaction: new vs old

            current_success_rate = current_metrics.get('success_rate_percent', 0)

            # For demo, assume new model is better if current < 95%
            should_deploy = current_success_rate < 95.0

            if should_deploy:
                logger.info(f"âœ… New model outperforms baseline (current: {current_success_rate:.1f}%)")
            else:
                logger.info(f"âš ï¸ Baseline model still better (current: {current_success_rate:.1f}%)")

            return should_deploy

        except Exception as e:
            logger.error(f"âŒ Comparison failed: {e}")
            return False

    def deploy_new_model(self):
        """Deploy the retrained model to production"""
        logger.info("ğŸš€ Step 5/5: Deploying new model...")

        try:
            # In production, this would:
            # 1. Build new Docker image with updated model
            # 2. Push to container registry (gcr.io)
            # 3. Deploy to Cloud Run with traffic splitting
            # 4. Gradually shift traffic: 10% â†’ 50% â†’ 100%
            # 5. Monitor for issues

            # For now, log the deployment command that would run
            deployment_command = """
            gcloud run deploy propbot-backend \\
              --source . \\
              --tag new-model-{job_id} \\
              --no-traffic \\
              --region us-central1

            # Then gradually shift traffic
            gcloud run services update-traffic propbot-backend \\
              --to-tags new-model-{job_id}=10
            """.format(job_id=self.retraining_job_id)

            logger.info(f"âœ… Deployment command prepared")
            logger.info(f"   In production, would execute: {deployment_command.strip()}")

            return True

        except Exception as e:
            logger.error(f"âŒ Deployment failed: {e}")
            return False

    def run_full_pipeline(self):
        """Execute the complete retraining pipeline"""
        logger.info("="*60)
        logger.info(f"ğŸ”„ STARTING RETRAINING PIPELINE: {self.retraining_job_id}")
        logger.info("="*60)

        # Send notification that retraining started
        self.notification_manager.send_retraining_alert({
            'job_id': self.retraining_job_id,
            'timestamp': datetime.now().isoformat(),
            'reasons': ['Automated retraining triggered']
        })

        # Step 1: Pull latest data
        if not self.pull_latest_data():
            logger.error("âŒ Pipeline failed at: Pull latest data")
            return False

        # Step 2: Retrain model
        if not self.retrain_model():
            logger.error("âŒ Pipeline failed at: Retrain model")
            return False

        # Step 3: Validate new model
        if not self.validate_new_model():
            logger.error("âŒ Pipeline failed at: Validate new model")
            return False

        # Step 4: Compare with baseline
        should_deploy = self.compare_with_baseline()

        # Step 5: Deploy if better
        if should_deploy:
            if self.deploy_new_model():
                logger.info("="*60)
                logger.info("âœ… RETRAINING PIPELINE COMPLETED SUCCESSFULLY")
                logger.info(f"   New model deployed: {self.retraining_job_id}")
                logger.info("="*60)

                # Send success notification
                self.notification_manager.send_deployment_success({
                    'job_id': self.retraining_job_id,
                    'version': 'retrained_model',
                    'timestamp': datetime.now().isoformat()
                })

                return True
            else:
                logger.error("âŒ Pipeline failed at: Deploy new model")
                return False
        else:
            logger.info("="*60)
            logger.info("â„¹ï¸ RETRAINING COMPLETED - Keeping existing model")
            logger.info("   New model did not outperform baseline")
            logger.info("="*60)
            return True


def main():
    """Main entry point for retraining pipeline"""
    pipeline = ModelRetrainingPipeline()
    success = pipeline.run_full_pipeline()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
